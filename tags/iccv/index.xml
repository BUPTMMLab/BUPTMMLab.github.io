<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Iccv on BUPTMMLab</title><link>https://BUPTMMLab.github.io/tags/iccv/</link><description>Recent content in Iccv on BUPTMMLab</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 26 Jun 2025 10:00:00 +0000</lastBuildDate><atom:link href="https://BUPTMMLab.github.io/tags/iccv/index.xml" rel="self" type="application/rss+xml"/><item><title>From Abyssal Darkness to Blinding Glare: A Benchmark on Extreme Exposure Correction in Real World</title><link>https://BUPTMMLab.github.io/publications/2025/from-abyssal-darkness-to-blinding-glare/</link><pubDate>Thu, 26 Jun 2025 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2025/from-abyssal-darkness-to-blinding-glare/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Exposure correction aims to restore over/under-exposed images to well-exposed ones using a single network. However, existing methods mainly handle non-extreme exposure conditions and struggle with the severe luminance and texture loss caused by extreme exposure. Through a thorough investigation, we find that the lack of high-quality benchmark datasets significantly limits progress in extreme exposure correction. To address this issue, we introduce the first Real-world Extreme Exposure Dataset, REED. By leveraging the burst shooting mode of cameras, we capture image sequences covering a luminance range from extremely dark to extremely bright. To prevent misalignment caused by camera motion and scene changes, we apply cropping and an improved SIFT algorithm to ensure precise alignment. We also propose a novel Context-Guided Luminance-Normalized Iterative Exposure Refinement Network. We employ Contrastive Loss and Luminance Normalizer to disentangle the coupled distribution of over/under-exposed images. In certain cases, luminance alone is insufficient for determining over/under-exposure, so we integrate semantic guidance into the Semantic-aware Exposure Diffusion Model to further enhance luminance and texture restoration. Inspired by the effectiveness of iterative correction in improving color and texture, we introduce the CLIP-Guided Iterative Refinement Strategy. Extensive experiments validate the superiority of our dataset and method. Our dataset and code will be publicly available.&lt;/p></description></item><item><title>Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement</title><link>https://BUPTMMLab.github.io/publications/2023/dancing-in-the-dark-a-benchmark-towards-general-low-light-video-enhancement/</link><pubDate>Sun, 08 Oct 2023 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2023/dancing-in-the-dark-a-benchmark-towards-general-low-light-video-enhancement/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Low-light video enhancement is a challenging task with broad applications. However, current research in this area is limited by the lack of high-quality benchmark datasets. To address this issue, we design a camera system and collect a high-quality low-light video dataset with multiple exposures and cameras. Our dataset provides dynamic video pairs with pronounced camera motion and strict spatial alignment. To achieve general low-light video enhancement, we also propose a novel Retinex-based method named Light Adjustable Network (LAN). LAN iteratively refines the illumination and adaptively adjusts it under varying lighting conditions, leading to visually appealing results even in diverse real-world scenarios. The extensive experiments demonstrate the superiority of our low-light video dataset and enhancement method. Our dataset is available at &lt;a href="https://github.com/ciki000/DID">https://github.com/ciki000/DID&lt;/a>.&lt;/p></description></item></channel></rss>