<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Acm-Multimedia on BUPTMMLab</title><link>https://BUPTMMLab.github.io/tags/acm-multimedia/</link><description>Recent content in Acm-Multimedia on BUPTMMLab</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 28 Oct 2024 10:00:00 +0000</lastBuildDate><atom:link href="https://BUPTMMLab.github.io/tags/acm-multimedia/index.xml" rel="self" type="application/rss+xml"/><item><title>Exploring in Extremely Dark: Low-Light Video Enhancement with Real Events</title><link>https://BUPTMMLab.github.io/publications/2024/exploring-in-extremely-dark-low-light-video-enhancement-with-real-events/</link><pubDate>Mon, 28 Oct 2024 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2024/exploring-in-extremely-dark-low-light-video-enhancement-with-real-events/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Due to the limitations of sensor, traditional cameras struggle to capture details within extremely dark areas of videos. The absence of such details can significantly impact the effectiveness of low-light video enhancement. In contrast, event cameras offer a visual representation with higher dynamic range, facilitating the capture of motion information even in exceptionally dark conditions. Motivated by this advantage, we propose the Real-Event Embedded Network for low-light video enhancement. To better utilize events for enhancing extremely dark regions, we propose an Event-Image Fusion module, which can identify these dark regions and enhance them significantly. To ensure temporal stability of the video and restore details within extremely dark areas, we design unsupervised temporal consistency loss and detail contrast loss. Alongside the supervised loss, these loss functions collectively contribute to the semi-supervised training of the network on unpaired real data. Experimental results on synthetic and real data demonstrate the superiority of the proposed method compared to the state-of-the-art methods.&lt;/p></description></item><item><title>Learning Exposure Correction in Dynamic Scenes</title><link>https://BUPTMMLab.github.io/publications/2024/learning-exposure-correction-in-dynamic-scenes/</link><pubDate>Mon, 28 Oct 2024 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2024/learning-exposure-correction-in-dynamic-scenes/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Exposure correction aims to enhance visual data suffering from improper exposures, which can greatly improve satisfactory visual effects. However, previous methods mainly focus on the image modality, and the video counterpart is less explored in the literature. Directly applying prior image-based methods to videos results in temporal incoherence with low visual quality. Through thorough investigation, we find that the development of relevant communities is limited by the absence of a benchmark dataset. Therefore, in this paper, we construct the first real-world paired video dataset, including both underexposure and overexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR cameras and a beam splitter to simultaneously capture improper and normal exposure videos. Additionally, we propose an end-to-end video exposure correction network, in which a dual-stream module is designed to deal with both underexposure and overexposure factors, enhancing the illumination based on Retinex theory. The extensive experiments based on various metrics and user studies demonstrate the significance of our dataset and the effectiveness of our method. The code and dataset are available at &lt;a href="https://github.com/kravrolens/VECNet">https://github.com/kravrolens/VECNet&lt;/a>.&lt;/p></description></item></channel></rss>