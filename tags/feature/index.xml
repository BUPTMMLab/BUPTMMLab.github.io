<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Feature on BUPTMMLab</title><link>https://BUPTMMLab.github.io/tags/feature/</link><description>Recent content in Feature on BUPTMMLab</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 26 Jun 2025 10:00:00 +0000</lastBuildDate><atom:link href="https://BUPTMMLab.github.io/tags/feature/index.xml" rel="self" type="application/rss+xml"/><item><title>From Abyssal Darkness to Blinding Glare: A Benchmark on Extreme Exposure Correction in Real World</title><link>https://BUPTMMLab.github.io/publications/2025/from-abyssal-darkness-to-blinding-glare/</link><pubDate>Thu, 26 Jun 2025 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2025/from-abyssal-darkness-to-blinding-glare/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Exposure correction aims to restore over/under-exposed images to well-exposed ones using a single network. However, existing methods mainly handle non-extreme exposure conditions and struggle with the severe luminance and texture loss caused by extreme exposure. Through a thorough investigation, we find that the lack of high-quality benchmark datasets significantly limits progress in extreme exposure correction. To address this issue, we introduce the first Real-world Extreme Exposure Dataset, REED. By leveraging the burst shooting mode of cameras, we capture image sequences covering a luminance range from extremely dark to extremely bright. To prevent misalignment caused by camera motion and scene changes, we apply cropping and an improved SIFT algorithm to ensure precise alignment. We also propose a novel Context-Guided Luminance-Normalized Iterative Exposure Refinement Network. We employ Contrastive Loss and Luminance Normalizer to disentangle the coupled distribution of over/under-exposed images. In certain cases, luminance alone is insufficient for determining over/under-exposure, so we integrate semantic guidance into the Semantic-aware Exposure Diffusion Model to further enhance luminance and texture restoration. Inspired by the effectiveness of iterative correction in improving color and texture, we introduce the CLIP-Guided Iterative Refinement Strategy. Extensive experiments validate the superiority of our dataset and method. Our dataset and code will be publicly available.&lt;/p></description></item></channel></rss>