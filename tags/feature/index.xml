<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Feature on BUPTMMLab</title><link>https://BUPTMMLab.github.io/tags/feature/</link><description>Recent content in Feature on BUPTMMLab</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 28 Oct 2024 10:00:00 +0000</lastBuildDate><atom:link href="https://BUPTMMLab.github.io/tags/feature/index.xml" rel="self" type="application/rss+xml"/><item><title>Learning Exposure Correction in Dynamic Scenes</title><link>https://BUPTMMLab.github.io/publications/2024/learning-exposure-correction-in-dynamic-scenes/</link><pubDate>Mon, 28 Oct 2024 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2024/learning-exposure-correction-in-dynamic-scenes/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Exposure correction aims to enhance visual data suffering from improper exposures, which can greatly improve satisfactory visual effects. However, previous methods mainly focus on the image modality, and the video counterpart is less explored in the literature. Directly applying prior image-based methods to videos results in temporal incoherence with low visual quality. Through thorough investigation, we find that the development of relevant communities is limited by the absence of a benchmark dataset. Therefore, in this paper, we construct the first real-world paired video dataset, including both underexposure and overexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR cameras and a beam splitter to simultaneously capture improper and normal exposure videos. Additionally, we propose an end-to-end video exposure correction network, in which a dual-stream module is designed to deal with both underexposure and overexposure factors, enhancing the illumination based on Retinex theory. The extensive experiments based on various metrics and user studies demonstrate the significance of our dataset and the effectiveness of our method. The code and dataset are available at &lt;a href="https://github.com/kravrolens/VECNet">https://github.com/kravrolens/VECNet&lt;/a>.&lt;/p></description></item></channel></rss>