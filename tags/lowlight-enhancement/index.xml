<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Lowlight-Enhancement on BUPTMMLab</title><link>https://BUPTMMLab.github.io/tags/lowlight-enhancement/</link><description>Recent content in Lowlight-Enhancement on BUPTMMLab</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 28 Oct 2024 10:00:00 +0000</lastBuildDate><atom:link href="https://BUPTMMLab.github.io/tags/lowlight-enhancement/index.xml" rel="self" type="application/rss+xml"/><item><title>Exploring in Extremely Dark: Low-Light Video Enhancement with Real Events</title><link>https://BUPTMMLab.github.io/publications/2024/exploring-in-extremely-dark-low-light-video-enhancement-with-real-events/</link><pubDate>Mon, 28 Oct 2024 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2024/exploring-in-extremely-dark-low-light-video-enhancement-with-real-events/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Due to the limitations of sensor, traditional cameras struggle to capture details within extremely dark areas of videos. The absence of such details can significantly impact the effectiveness of low-light video enhancement. In contrast, event cameras offer a visual representation with higher dynamic range, facilitating the capture of motion information even in exceptionally dark conditions. Motivated by this advantage, we propose the Real-Event Embedded Network for low-light video enhancement. To better utilize events for enhancing extremely dark regions, we propose an Event-Image Fusion module, which can identify these dark regions and enhance them significantly. To ensure temporal stability of the video and restore details within extremely dark areas, we design unsupervised temporal consistency loss and detail contrast loss. Alongside the supervised loss, these loss functions collectively contribute to the semi-supervised training of the network on unpaired real data. Experimental results on synthetic and real data demonstrate the superiority of the proposed method compared to the state-of-the-art methods.&lt;/p></description></item><item><title>Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement</title><link>https://BUPTMMLab.github.io/publications/2023/dancing-in-the-dark-a-benchmark-towards-general-low-light-video-enhancement/</link><pubDate>Sun, 08 Oct 2023 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2023/dancing-in-the-dark-a-benchmark-towards-general-low-light-video-enhancement/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Low-light video enhancement is a challenging task with broad applications. However, current research in this area is limited by the lack of high-quality benchmark datasets. To address this issue, we design a camera system and collect a high-quality low-light video dataset with multiple exposures and cameras. Our dataset provides dynamic video pairs with pronounced camera motion and strict spatial alignment. To achieve general low-light video enhancement, we also propose a novel Retinex-based method named Light Adjustable Network (LAN). LAN iteratively refines the illumination and adaptively adjusts it under varying lighting conditions, leading to visually appealing results even in diverse real-world scenarios. The extensive experiments demonstrate the superiority of our low-light video dataset and enhancement method. Our dataset is available at &lt;a href="https://github.com/ciki000/DID">https://github.com/ciki000/DID&lt;/a>.&lt;/p></description></item><item><title>You Do Not Need Additional Priors or Regularizers in Retinex-based Low-light Image Enhancement</title><link>https://BUPTMMLab.github.io/publications/2023/you-do-not-need-additional-priors-or-regularizers-in-retinex-based-low-light-image-enhancement/</link><pubDate>Tue, 20 Jun 2023 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2023/you-do-not-need-additional-priors-or-regularizers-in-retinex-based-low-light-image-enhancement/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Images captured in low-light conditions often suffer from significant quality degradation. Recent works have built a large variety of deep Retinex-based networks to enhance low-light images. The Retinex-based methods require decomposing the image into reflectance and illumination components, which is a highly ill-posed problem and there is no available ground truth. Previous works addressed this problem by imposing some additional priors or regularizers. However, finding an effective prior or regularizer that can be applied in various scenes is challenging, and the performance of the model suffers from too many additional constraints. We propose a contrastive learning method and a self-knowledge distillation method for Retinex decomposition that allow training our Retinex-based model without elaborate hand-crafted regularization functions. Rather than estimating reflectance and illuminance images and representing the final images as their element-wise products as in previous works, our regularizer-free Retinex decomposition and synthesis network (RFR) extracts reflectance and illuminance features and synthesizes them end-to-end. In addition, we propose a loss function for contrastive learning and a progressive learning strategy for self-knowledge distillation. Extensive experimental results demonstrate that our proposed methods can achieve superior performance compared with state-of-the-art approaches.&lt;/p></description></item></channel></rss>