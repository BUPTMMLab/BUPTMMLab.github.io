<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cvpr on BUPTMMLab</title><link>https://BUPTMMLab.github.io/tags/cvpr/</link><description>Recent content in Cvpr on BUPTMMLab</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Tue, 20 Jun 2023 10:00:00 +0000</lastBuildDate><atom:link href="https://BUPTMMLab.github.io/tags/cvpr/index.xml" rel="self" type="application/rss+xml"/><item><title>You Do Not Need Additional Priors or Regularizers in Retinex-based Low-light Image Enhancement</title><link>https://BUPTMMLab.github.io/publications/2023/you-do-not-need-additional-priors-or-regularizers-in-retinex-based-low-light-image-enhancement/</link><pubDate>Tue, 20 Jun 2023 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2023/you-do-not-need-additional-priors-or-regularizers-in-retinex-based-low-light-image-enhancement/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Images captured in low-light conditions often suffer from significant quality degradation. Recent works have built a large variety of deep Retinex-based networks to enhance low-light images. The Retinex-based methods require decomposing the image into reflectance and illumination components, which is a highly ill-posed problem and there is no available ground truth. Previous works addressed this problem by imposing some additional priors or regularizers. However, finding an effective prior or regularizer that can be applied in various scenes is challenging, and the performance of the model suffers from too many additional constraints. We propose a contrastive learning method and a self-knowledge distillation method for Retinex decomposition that allow training our Retinex-based model without elaborate hand-crafted regularization functions. Rather than estimating reflectance and illuminance images and representing the final images as their element-wise products as in previous works, our regularizer-free Retinex decomposition and synthesis network (RFR) extracts reflectance and illuminance features and synthesizes them end-to-end. In addition, we propose a loss function for contrastive learning and a progressive learning strategy for self-knowledge distillation. Extensive experimental results demonstrate that our proposed methods can achieve superior performance compared with state-of-the-art approaches.&lt;/p></description></item></channel></rss>