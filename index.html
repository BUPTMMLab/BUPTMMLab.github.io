<!doctype html><html lang=en-us dir=ltr><head><meta name=generator content="Hugo 0.147.7"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>BUPTMMLab</title><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.0/css/all.min.css><meta name=description content><link rel=stylesheet href=/css/main.min.df34b0153fa1319eab0404d6d67f5c8cf3f0416e02003d820402c72b72984952.css integrity="sha256-3zSwFT+hMZ6rBATW1n9cjPPwQW4CAD2CBALHK3KYSVI=" crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.min.f6abb61f6b9b2e784eba22dfb93cd399ce30ee01825791830a2737d6bfcd2be9.css integrity="sha256-9qu2H2ubLnhOuiLfuTzTmc4w7gGCV5GDCic31r/NK+k=" crossorigin=anonymous><link rel=stylesheet href=/css/fontawesome-free/fontawesome.min.6fc54ad7f858066d7a20163a3700a27bf0d0db05f352e455209505c9f1cf0691.css integrity="sha256-b8VK1/hYBm16IBY6NwCie/DQ2wXzUuRVIJUFyfHPBpE=" crossorigin=anonymous><button id=backToTop class="w-12 h-12 fixed bottom-6 right-6 z-50 p-3 rounded-full bg-blue-600 text-white shadow-md hover:bg-blue-700 transition" aria-label="Back to top">
<i class="fa-solid fa-arrow-up"></i></button></head><body><header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col"><div class="flex items-center justify-between"><div class="flex items-center space-x-4"><img src=/images/BUPT_logo.png alt=Logo class="h-14 max-w-[20rem] object-contain"><div class="relative rounded-full py-1.5 px-6 hover:bg-zinc-200 text-xl font-bold uppercase"><h1><a class="text-4xl before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto text-transparent bg-clip-text bg-gradient-to-r from-blue-500 to-purple-600" href=https://BUPTMMLab.github.io/>BUPTMMLab</a></h1></div></div><div class="flex items-center space-x-4"><a href=/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Home
</a><a href=/publications/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Publications
</a><a href=/tags/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Tags
</a><a href=/members/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Members
</a><a href=/about/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">About</a></div></div></div></header><main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><section class="block md:flex relative group p-6 lg:p-10 space-y-6 md:space-x-6 my-14 bg-zinc-100 rounded-3xl hover:bg-blue-100"><figure class="basis-1/2 w-full aspect-video overflow-hidden rounded-2xl border"><img class="w-full h-full object-cover object-center group-hover:scale-105 transition duration-500 cursor-pointer" src=https://BUPTMMLab.github.io/images/pubs/ICCV2025wangFromAbyssal/wang_iccv_networkarch_hu_5c6ce7fb48d8fc09.png alt=BUPTMMLab></figure><div class="basis-1/2 self-center space-y-3"><div class="flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-800 text-l font-semibold px-3 py-1 rounded-full">ICCV</span></div><h2 class="text-2xl md:text-3xl lg:text-5xl font-bold"><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto" href=https://BUPTMMLab.github.io/publications/2025/from-abyssal-darkness-to-blinding-glare/>From Abyssal Darkness to Blinding Glare: A Benchmark on Extreme Exposure Correction in Real World</a></h2><p class="text-gray-700 text-base font-medium"><span class=text-l>Bo Wang</span>,
<a href=https://teacher.bupt.edu.cn/fuhuiyuan class="text-blue-600 hover:underline text-l">Huiyuan Fu</a>,
<span class=text-l>Zhiye Huang</span>,
<span class=text-l>Siru Zhang</span>,
<span class=text-l>Xin Wang</span>,
<a href="https://scholar.google.com/citations?user=A-vcjvUAAAAJ" class="text-blue-600 hover:underline text-l">Huadong Ma</a></p><time datetime=2025-06-26T10:00:00+00:00><span class=font-bold>Jun 26, 2025</span></time></div></section><section class=mb-16><div class="flex items-center mb-6"><h2 class="text-3xl md:text-4xl font-bold mr-auto">Exposure Correction</h2><a class="border rounded-full py-2 px-4 md:px-6 hover:bg-blue-100" href=https://BUPTMMLab.github.io/publications/exposure_correction>View All</a></div><div class="grid grid-cols-1 md:grid-cols-3 gap-x-6 gap-y-10"><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl"><a class=insert-link href=https://BUPTMMLab.github.io/publications/2025/from-abyssal-darkness-to-blinding-glare/></a><figure class="w-full aspect-video overflow-hidden rounded-3xl"><img class="w-full h-full object-cover object-center group-hover:scale-105 transition duration-500 cursor-pointer" src=https://BUPTMMLab.github.io/images/pubs/ICCV2025wangFromAbyssal/wang_iccv_networkarch_hu_7d15dfcbab85fd56.png alt="From Abyssal Darkness to Blinding Glare: A Benchmark on Extreme Exposure Correction in Real World"></figure><div class=p-6><div class="flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full">ICCV</span>
<span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full"><time datetime=2025-06-26T10:00:00+00:00>2025</time></span></div><h3 class="my-4 text-2xl font-bold">From Abyssal Darkness to Blinding Glare: A Benchmark on Extreme Exposure Correction in Real World</h3><p class="text-normal leading-normal text-zinc-500 line-clamp-2">This paper introduces Real-world Extreme Exposure Dataset (REED) to improve extreme exposure correction in real world scenarios. The method is based on burst capturing with a range of exposures and accurate SIFT-based image alignment. The paper also introduces a method (CLIER) for extreme exposure correction based on luminance normalization, semantic awareness, diffusion, and iterative refinement. The experiments validate the efficacy of the proposed method.</p></div></article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl"><a class=insert-link href=https://BUPTMMLab.github.io/publications/2024/learning-exposure-correction-in-dynamic-scenes/></a><figure class="w-full aspect-video overflow-hidden rounded-3xl"><img class="w-full h-full object-cover object-center group-hover:scale-105 transition duration-500 cursor-pointer" src=https://BUPTMMLab.github.io/images/pubs/MM2024liuLearningExposure/liu_acmmm2024_DIME_hu_dfc0ef2f270aa18f.png alt="Learning Exposure Correction in Dynamic Scenes"></figure><div class=p-6><div class="flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full">ACM Multimedia</span>
<span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full">Oral</span>
<span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full"><time datetime=2024-10-28T10:00:00+00:00>2024</time></span></div><h3 class="my-4 text-2xl font-bold">Learning Exposure Correction in Dynamic Scenes</h3><p class="text-normal leading-normal text-zinc-500 line-clamp-2">This paper constructs the first real-world paired video dataset DIME, including both underexposure and overexposure dynamic scenes, and proposes an end-to-end video exposure correction network, in which a dual-stream module is designed to deal with both underexposure and overexposure factors.</p></div></article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl"><a class=insert-link href=https://BUPTMMLab.github.io/publications/2024/region-aware-exposure-consistency-network-for-mixed-exposure-correction/></a><figure class="w-full aspect-video overflow-hidden rounded-3xl"><img class="w-full h-full object-cover object-center group-hover:scale-105 transition duration-500 cursor-pointer" src=https://BUPTMMLab.github.io/images/pubs/AAAI2024liuRegionAwareExposure/liu_aaai2024_RECNet_hu_c4a24e566a4099e3.png alt="Region-Aware Exposure Consistency Network for Mixed Exposure Correction"></figure><div class=p-6><div class="flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full">AAAI</span>
<span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full"><time datetime=2024-02-28T10:00:00+00:00>2024</time></span></div><h3 class="my-4 text-2xl font-bold">Region-Aware Exposure Consistency Network for Mixed Exposure Correction</h3><p class="text-normal leading-normal text-zinc-500 line-clamp-2">An effective Region-aware Exposure Correction Network (RECNet) is introduced that can handle mixed exposure by adaptively learning and bridging different regional exposure representations and an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity is proposed.</p></div></article></div></section><section class=mb-16><div class="flex items-center mb-6"><h2 class="text-3xl md:text-4xl font-bold mr-auto">Lowlight Enhancement</h2><a class="border rounded-full py-2 px-4 md:px-6 hover:bg-blue-100" href=https://BUPTMMLab.github.io/publications/lowlight_enhancement>View All</a></div><div class="grid grid-cols-1 md:grid-cols-3 gap-x-6 gap-y-10"><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl"><a class=insert-link href=https://BUPTMMLab.github.io/publications/2024/exploring-in-extremely-dark-low-light-video-enhancement-with-real-events/></a><figure class="w-full aspect-video overflow-hidden rounded-3xl"><img class="w-full h-full object-cover object-center group-hover:scale-105 transition duration-500 cursor-pointer" src=https://BUPTMMLab.github.io/images/pubs/MM2024wangExploringExtremely/wang_mm_networkarch_hu_58480d0a25ed6407.png alt="Exploring in Extremely Dark: Low-Light Video Enhancement with Real Events"></figure><div class=p-6><div class="flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full">ACMMM</span>
<span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full"><time datetime=2024-10-28T10:00:00+00:00>2024</time></span></div><h3 class="my-4 text-2xl font-bold">Exploring in Extremely Dark: Low-Light Video Enhancement with Real Events</h3><p class="text-normal leading-normal text-zinc-500 line-clamp-2">This paper proposes the Real-Event Embedded Network (REN) for low-light video enhancement using real events to restore details in extremely dark areas. It introduces the Event-Image Fusion module and unsupervised losses for semi-supervised training on unpaired data. Experiments demonstrate superiority over state-of-the-art methods.</p></div></article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl"><a class=insert-link href=https://BUPTMMLab.github.io/publications/2023/dancing-in-the-dark-a-benchmark-towards-general-low-light-video-enhancement/></a><figure class="w-full aspect-video overflow-hidden rounded-3xl"><img class="w-full h-full object-cover object-center group-hover:scale-105 transition duration-500 cursor-pointer" src=https://BUPTMMLab.github.io/images/pubs/ICCV2023fuDancingDark/fu_iccv_networkarch_hu_1e462f6791038fdb.png alt="Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement"></figure><div class=p-6><div class="flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full">ICCV</span>
<span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full"><time datetime=2023-10-04T10:00:00+00:00>2023</time></span></div><h3 class="my-4 text-2xl font-bold">Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement</h3><p class="text-normal leading-normal text-zinc-500 line-clamp-2">This paper introduces a high-quality low-light video dataset (DID) and a Retinex-based method called Light Adjustable Network (LAN) for general low-light video enhancement. The dataset features dynamic videos with multiple exposures and cameras, while LAN iteratively refines illumination for adaptive enhancement.</p></div></article><article class="relative group bg-zinc-100 hover:bg-blue-100 rounded-3xl"><a class=insert-link href=https://BUPTMMLab.github.io/publications/2023/you-do-not-need-additional-priors-or-regularizers-in-retinex-based-low-light-image-enhancement/></a><figure class="w-full aspect-video overflow-hidden rounded-3xl"><img class="w-full h-full object-cover object-center group-hover:scale-105 transition duration-500 cursor-pointer" src=https://BUPTMMLab.github.io/images/pubs/CVPR2023FuYouDoNotNeed/fu_cvpr_visual_comparison_hu_b7f035eed898c23d.png alt="You Do Not Need Additional Priors or Regularizers in Retinex-based Low-light Image Enhancement"></figure><div class=p-6><div class="flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full">CVPR</span>
<span class="bg-blue-100 text-blue-800 text-sm font-semibold px-3 py-1 rounded-full"><time datetime=2023-06-20T10:00:00+00:00>2023</time></span></div><h3 class="my-4 text-2xl font-bold">You Do Not Need Additional Priors or Regularizers in Retinex-based Low-light Image Enhancement</h3><p class="text-normal leading-normal text-zinc-500 line-clamp-2">This paper proposes a regularizer-free Retinex decomposition and synthesis network (RFR) for low-light image enhancement. It introduces a contrastive learning method and a self-knowledge distillation method to train the model without additional priors or regularizers. The approach extracts reflectance and illumination features and synthesizes them end-to-end, achieving superior performance on various datasets.</p></div></article></div></section><hr class=my-8><section class=mb-16><div class="flex items-center mb-6"><h2 class="text-3xl md:text-4xl font-bold mr-auto">Recent Publications</h2></div><div class="relative flex items-center"><div class="bg-gray-200 border border-gray-300 text-gray-400 rounded-full px-4 py-2 cursor-not-allowed"><i class="fa-solid fa-arrow-left"></i></div><div class="flex-1 px-2"><div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6"><div class="bg-white shadow-md rounded-lg p-4 hover:shadow-lg transition-shadow duration-300"><img src=/images/pubs/ICCV2025wangFromAbyssal/wang_iccv_networkarch.png alt="From Abyssal Darkness to Blinding Glare: A Benchmark on Extreme Exposure Correction in Real World" class="w-full h-48 object-cover object-center rounded-md mb-4"><h2 class="text-lg font-semibold"><a href=/publications/2025/from-abyssal-darkness-to-blinding-glare/ class="text-blue-600 hover:underline">From Abyssal Darkness to Blinding Glare: A Benchmark on Extreme Exposure Correction in Real World</a></h2><p class="text-sm text-gray-500 mb-1">Jun 26, 2025</p><div class="prose prose-sm max-h-20 overflow-hidden" title="This paper introduces Real-world Extreme Exposure Dataset (REED) to improve extreme exposure correction in real world scenarios. The method is based on burst capturing with a range of exposures and accurate SIFT-based image alignment. The paper also introduces a method (CLIER) for extreme exposure correction based on luminance normalization, semantic awareness, diffusion, and iterative refinement. The experiments validate the efficacy of the proposed method.">This paper introduces Real-world Extreme Exposure Dataset (REED) to improve extreme exposure...</div><div class="mt-2 flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">exposure correction</span>
<span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">iccv</span>
<span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">feature</span></div></div><div class="bg-white shadow-md rounded-lg p-4 hover:shadow-lg transition-shadow duration-300"><img src=/images/pubs/MM2024wangExploringExtremely/wang_mm_networkarch.png alt="Exploring in Extremely Dark: Low-Light Video Enhancement with Real Events" class="w-full h-48 object-cover object-center rounded-md mb-4"><h2 class="text-lg font-semibold"><a href=/publications/2024/exploring-in-extremely-dark-low-light-video-enhancement-with-real-events/ class="text-blue-600 hover:underline">Exploring in Extremely Dark: Low-Light Video Enhancement with Real Events</a></h2><p class="text-sm text-gray-500 mb-1">Oct 28, 2024</p><div class="prose prose-sm max-h-20 overflow-hidden" title="This paper proposes the Real-Event Embedded Network (REN) for low-light video enhancement using real events to restore details in extremely dark areas. It introduces the Event-Image Fusion module and unsupervised losses for semi-supervised training on unpaired data. Experiments demonstrate superiority over state-of-the-art methods.">This paper proposes the Real-Event Embedded Network (REN) for low-light video enhancement using real...</div><div class="mt-2 flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">lowlight enhancement</span>
<span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">acm multimedia</span>
<span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">event</span></div></div><div class="bg-white shadow-md rounded-lg p-4 hover:shadow-lg transition-shadow duration-300"><img src=/images/pubs/MM2024liuLearningExposure/liu_acmmm2024_DIME.png alt="Learning Exposure Correction in Dynamic Scenes" class="w-full h-48 object-cover object-center rounded-md mb-4"><h2 class="text-lg font-semibold"><a href=/publications/2024/learning-exposure-correction-in-dynamic-scenes/ class="text-blue-600 hover:underline">Learning Exposure Correction in Dynamic Scenes</a></h2><p class="text-sm text-gray-500 mb-1">Oct 28, 2024</p><div class="prose prose-sm max-h-20 overflow-hidden" title="This paper constructs the first real-world paired video dataset DIME, including both underexposure and overexposure dynamic scenes, and proposes an end-to-end video exposure correction network, in which a dual-stream module is designed to deal with both underexposure and overexposure factors.">This paper constructs the first real-world paired video dataset DIME, including both underexposure...</div><div class="mt-2 flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">exposure correction</span>
<span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">acm multimedia</span></div></div><div class="bg-white shadow-md rounded-lg p-4 hover:shadow-lg transition-shadow duration-300"><img src=/images/pubs/AAAI2024liuRegionAwareExposure/liu_aaai2024_RECNet.png alt="Region-Aware Exposure Consistency Network for Mixed Exposure Correction" class="w-full h-48 object-cover object-center rounded-md mb-4"><h2 class="text-lg font-semibold"><a href=/publications/2024/region-aware-exposure-consistency-network-for-mixed-exposure-correction/ class="text-blue-600 hover:underline">Region-Aware Exposure Consistency Network for Mixed Exposure Correction</a></h2><p class="text-sm text-gray-500 mb-1">Feb 28, 2024</p><div class="prose prose-sm max-h-20 overflow-hidden" title="An effective Region-aware Exposure Correction Network (RECNet) is introduced that can handle mixed exposure by adaptively learning and bridging different regional exposure representations and an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity is proposed.">An effective Region-aware Exposure Correction Network (RECNet) is introduced that can handle mixed...</div><div class="mt-2 flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">exposure correction</span>
<span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">aaai</span></div></div><div class="bg-white shadow-md rounded-lg p-4 hover:shadow-lg transition-shadow duration-300"><img src=/images/pubs/ICCV2023fuDancingDark/fu_iccv_networkarch.png alt="Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement" class="w-full h-48 object-cover object-center rounded-md mb-4"><h2 class="text-lg font-semibold"><a href=/publications/2023/dancing-in-the-dark-a-benchmark-towards-general-low-light-video-enhancement/ class="text-blue-600 hover:underline">Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement</a></h2><p class="text-sm text-gray-500 mb-1">Oct 4, 2023</p><div class="prose prose-sm max-h-20 overflow-hidden" title="This paper introduces a high-quality low-light video dataset (DID) and a Retinex-based method called Light Adjustable Network (LAN) for general low-light video enhancement. The dataset features dynamic videos with multiple exposures and cameras, while LAN iteratively refines illumination for adaptive enhancement.">This paper introduces a high-quality low-light video dataset (DID) and a Retinex-based method called...</div><div class="mt-2 flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">lowlight enhancement</span>
<span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">iccv</span>
<span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">dataset</span></div></div><div class="bg-white shadow-md rounded-lg p-4 hover:shadow-lg transition-shadow duration-300"><img src=/images/pubs/CVPR2023FuYouDoNotNeed/fu_cvpr_visual_comparison.png alt="You Do Not Need Additional Priors or Regularizers in Retinex-based Low-light Image Enhancement" class="w-full h-48 object-cover object-center rounded-md mb-4"><h2 class="text-lg font-semibold"><a href=/publications/2023/you-do-not-need-additional-priors-or-regularizers-in-retinex-based-low-light-image-enhancement/ class="text-blue-600 hover:underline">You Do Not Need Additional Priors or Regularizers in Retinex-based Low-light Image Enhancement</a></h2><p class="text-sm text-gray-500 mb-1">Jun 20, 2023</p><div class="prose prose-sm max-h-20 overflow-hidden" title="This paper proposes a regularizer-free Retinex decomposition and synthesis network (RFR) for low-light image enhancement. It introduces a contrastive learning method and a self-knowledge distillation method to train the model without additional priors or regularizers. The approach extracts reflectance and illumination features and synthesizes them end-to-end, achieving superior performance on various datasets.">This paper proposes a regularizer-free Retinex decomposition and synthesis network (RFR) for...</div><div class="mt-2 flex flex-wrap gap-2"><span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">lowlight enhancement</span>
<span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">cvpr</span>
<span class="bg-blue-100 text-blue-700 text-xs font-semibold px-2 py-1 rounded-full uppercase">retinex</span></div></div></div></div><div class="bg-gray-200 border border-gray-300 text-gray-400 rounded-full px-4 py-2 cursor-not-allowed"><i class="fa-solid fa-arrow-right"></i></div></div><div class="mt-6 text-center text-sm font-semibold text-gray-700">Page 1 / 1</div></section></main><footer class="bg-zinc-100 py-10 md:py-14"><script>const btn=document.getElementById("backToTop");window.addEventListener("scroll",()=>{btn.classList.toggle("hidden",window.scrollY<100)}),btn.addEventListener("click",()=>{window.scrollTo({top:0,behavior:"smooth"})})</script><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div class="flex flex-wrap space-y-6 mb-4"><div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10"><a class="flex items-center group" href=https://BUPTMMLab.github.io/><span class="text-4xl font-semibold uppercase">BUPTMMLab</span></a><p class=font-semibold>Research on Multimedia and Multimodal Learning.<br>Multimedia Lab, Beijing University of Posts and Telecommunications.</p></div><div class="self-center flex flex-col w-full md:w-2/5"><ul id=social-media class="flex items-center space-x-4"><li><a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href=https://github.com/BUPTMMLab target=_blank rel="noopener noreferrer"><i class="fa-brands fa-github fa-xl"></i></a></li></ul></div></div><div class=my-8><ul class="flex items-center space-x-4"><li><a class="decoration-auto hover:underline font-semibold" href=/>Home</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/publications/>Publications</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/tags/>Tags</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/members/>Members</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/about/>About</a></li></ul></div><div class="border-t pt-4"><p class=text-sm>Copyright © 2025. All rights reserved.</p></div></div><div class="text-center text-gray-500 text-sm mt-4 space-x-4"><span>本站总访问量 <span id=busuanzi_value_site_pv>0</span> 次</span>
<span>访客人数 <span id=busuanzi_value_site_uv>0</span> 人</span></div><script src=/js/busuanzi.pure.mini.min.1c1e7fbaa98df67c212645a84aa3b7cbb1e5aeca16c576b935d9b8b89b6d7c55.js integrity="sha256-HB5/uqmN9nwhJkWoSqO3y7HlrsoWxXa5Ndm4uJttfFU=" defer></script></footer><script defer src=/js/insertoggle.474f9b0e08021c6519cff4e46df14ccf148285b2d3a23d6321d6e10f25c291fb.js integrity="sha256-R0+bDggCHGUZz/TkbfFMzxSChbLToj1jIdbhDyXCkfs=" crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){var e=document.createElement("div");e.className="adsbox",e.style.height="1px",e.style.position="absolute",e.style.top="-1000px",document.body.appendChild(e),window.setTimeout(function(){var t,n=e.offsetHeight===0;document.body.removeChild(e),n&&(t=document.createElement("div"),t.id="ublock-warning",t.style.position="fixed",t.style.top="0",t.style.left="0",t.style.width="100%",t.style.padding="12px",t.style.backgroundColor="#ffcc00",t.style.color="#000",t.style.fontSize="16px",t.style.textAlign="center",t.style.zIndex="9999",t.style.opacity="0",t.style.transition="opacity 1s ease",t.innerHTML="Detect the AD blocker. For a better browsing experience, please consider disabling uBlock Origin or other ad blockers on this site. Thank you!",document.body.appendChild(t),setTimeout(function(){t.style.opacity="1"},100),setTimeout(function(){t.style.opacity="0",setTimeout(function(){t.remove()},1e3)},5e3))},100)})</script></body></html>