<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement | BUPTMMLab</title><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.0/css/all.min.css><meta name=description content="This paper introduces a high-quality low-light video dataset (DID) and a Retinex-based method called Light Adjustable Network (LAN) for general low-light video enhancement. The dataset features dynamic videos with multiple exposures and cameras, while LAN iteratively refines illumination for adaptive enhancement."><link rel=stylesheet href=/css/main.min.df34b0153fa1319eab0404d6d67f5c8cf3f0416e02003d820402c72b72984952.css integrity="sha256-3zSwFT+hMZ6rBATW1n9cjPPwQW4CAD2CBALHK3KYSVI=" crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.min.f6abb61f6b9b2e784eba22dfb93cd399ce30ee01825791830a2737d6bfcd2be9.css integrity="sha256-9qu2H2ubLnhOuiLfuTzTmc4w7gGCV5GDCic31r/NK+k=" crossorigin=anonymous><link rel=stylesheet href=/css/fontawesome-free/fontawesome.min.6fc54ad7f858066d7a20163a3700a27bf0d0db05f352e455209505c9f1cf0691.css integrity="sha256-b8VK1/hYBm16IBY6NwCie/DQ2wXzUuRVIJUFyfHPBpE=" crossorigin=anonymous><button id=backToTop class="w-12 h-12 fixed bottom-6 right-6 z-50 p-3 rounded-full bg-blue-600 text-white shadow-md hover:bg-blue-700 transition" aria-label="Back to top">
<i class="fa-solid fa-arrow-up"></i></button></head><body><header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col"><div class="flex items-center justify-between"><div class="flex items-center space-x-4"><img src=/images/BUPT_logo.png alt=Logo class="h-14 max-w-[20rem] object-contain"><div class="relative rounded-full py-1.5 px-6 hover:bg-zinc-200 text-xl font-bold uppercase"><h2><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto text-4xl text-transparent bg-clip-text bg-gradient-to-r from-blue-500 to-purple-600" href=https://BUPTMMLab.github.io/>BUPTMMLab</a></h2></div></div><div class="flex items-center space-x-4"><a href=/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Home
</a><a href=/publications/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Publications
</a><a href=/tags/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Tags
</a><a href=/members/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Members
</a><a href=/about/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">About</a></div></div></div></header><main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div class="items-center justify-center"><article><header class="flex flex-col justify-center items-center mx-auto"><div class="flex flex-wrap justify-center gap-6 mb-2"><span class="inline-block text-white font-semibold px-4 py-1 rounded-lg text-2xl shadow-md select-none" style="background:linear-gradient(to right,#f472b6,#8b5cf6,#3b82f6);-webkit-background-clip:text;-webkit-text-fill-color:transparent">ICCV</span></div><h1 id=title class="text-4xl text-center font-bold leading-normal">Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement</h1><div class="text-gray-700 text-base font-medium flex flex-wrap justify-center mt-4"><a href=https://teacher.bupt.edu.cn/fuhuiyuan class="text-blue-600 hover:underline">Huiyuan Fu</a>
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">1</sup>,&nbsp;
Wenkai Zheng
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">1</sup>,&nbsp;
Xicong Wang
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">1</sup>,&nbsp;
Jiaxuan Wang
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">1</sup>,&nbsp;
Heng Zhang
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">2</sup>,&nbsp;
<a href="https://scholar.google.com/citations?user=A-vcjvUAAAAJ" class="text-blue-600 hover:underline">Huadong Ma</a>
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">1</sup></div><div class="mt-2 text-gray-600 text-sm text-center"><div><sup>1</sup> Beijing University of Posts and Telecommunications</div><div><sup>2</sup> Xiaomi</div></div><div id=writer class="flex flex-col space-y-2"><span class="text-gray-700 text-base font-medium before:mr-2 before:opacity-50 space-x-4"><time datetime=2023-10-04T10:00:00+00:00>October 4, 2023</time></span></div><div class="flex flex-wrap justify-center gap-2 mt-6"><div><a href=https://openaccess.thecvf.com/content/ICCV2023/html/Fu_Dancing_in_the_Dark_A_Benchmark_towards_General_Low-light_Video_ICCV_2023_paper.html class="inline-block flex items-center gap-2 bg-blue-100 text-blue-800 text-m font-semibold px-3 py-1 rounded-full hover:bg-blue-200 transition"><i class="fas fa-file-pdf"></i>
<span>Paper</span></a></div><div><a href=https://github.com/ciki000/DID class="flex items-center gap-2 bg-blue-100 text-blue-800 text-m font-semibold px-3 py-1 rounded-full hover:bg-blue-200 transition"><i class="fa-solid fa-database"></i>
<span>Dataset</span></a></div></div><br><div id=tldr-box class="bg-white border border-gray-300 rounded-xl shadow-md p-4 hover:shadow-xl transition"><p><strong>TL;DR:</strong> This paper introduces a high-quality low-light video dataset (DID) and a Retinex-based method called Light Adjustable Network (LAN) for general low-light video enhancement. The dataset features dynamic videos with multiple exposures and cameras, while LAN iteratively refines illumination for adaptive enhancement.</p></div><br></header><figure id=featureimage class="flex justify-center"><img class="rounded-lg max-h-[500px] object-contain" src=https://BUPTMMLab.github.io/images/pubs/ICCV2023fuDancingDark/fu_iccv_networkarch_hu_38c461490e68e8e3.png alt width=750 height=440></figure><div id=content class="prose max-w-none lg:prose-lg mb-8"><h2 id=abstract>Abstract</h2><p>Low-light video enhancement is a challenging task with broad applications. However, current research in this area is limited by the lack of high-quality benchmark datasets. To address this issue, we design a camera system and collect a high-quality low-light video dataset with multiple exposures and cameras. Our dataset provides dynamic video pairs with pronounced camera motion and strict spatial alignment. To achieve general low-light video enhancement, we also propose a novel Retinex-based method named Light Adjustable Network (LAN). LAN iteratively refines the illumination and adaptively adjusts it under varying lighting conditions, leading to visually appealing results even in diverse real-world scenarios. The extensive experiments demonstrate the superiority of our low-light video dataset and enhancement method. Our dataset is available at <a href=https://github.com/ciki000/DID>https://github.com/ciki000/DID</a>.</p><h2 id=optical-system-to-collect-dataset>Optical System to Collect Dataset</h2><div class="flex justify-center w-full not-prose"><figure class="w-full p-2 bg-gray-50 rounded-md shadow-sm text-center"><div class="inline-block m-0 w-2/5"><img src=/images/pubs/ICCV2023fuDancingDark/fu_iccv_datasetsystem.png alt="Optical system to collect DID dataset" class="w-full h-auto object-cover rounded-md"></div><figcaption class="mt-2 text-m text-gray-600">The camera system consists of 5 capture devices (Sony RX100 M4, Canon EOS R10, Panasonic G9, Fujifilm XT4, Nikon Z5), an electric gimbal, a signal generator, and a central processing device. It collects paired low/normal-light videos by shooting frame by frame, adjusting camera ISO to capture low-light and normal-light frames at the same location, checking and synthesizing frames, and introducing slight gimbal movements (less than 1Â° horizontal + vertical) between pairs to ensure dynamic motion and continuity.</figcaption></figure></div></div><div class="mt-12 mb-10 bg-white rounded-xl shadow-md" id=BibTeX><div class="flex items-center justify-between px-4 py-4 bg-white border-b border-gray-300"><span class="text-xl font-semibold text-black">BibTeX</span>
<button onclick=copyBibtex() class="text-sm bg-gray-200 hover:bg-gray-300 px-3 py-1 rounded flex items-center gap-2 text-black">
<i class="fa-solid fa-copy"></i>
<span>Copy</span></button></div><pre class="mt-3 px-4 py-2 bg-white text-black text-sm overflow-x-auto whitespace-pre-wrap break-words leading-snug">
<code id=bibtex-content style=white-space:pre class="block whitespace-pre-line break-words text-black">@inproceedings{fuDancingDarkICCV2023,
  author={Fu, Huiyuan and Zheng, Wenkai and Wang, Xicong and Wang, Jiaxuan and Zhang, Heng and Ma, Huadong},
  booktitle={IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement}, 
  year={2023},
  pages={12831-12840}
}
</code>    
</pre></div><style>.toast{position:fixed;bottom:1.5rem;right:1.5rem;z-index:9999;background-color:#1f2937;color:#fff;padding:.75rem 1rem;border-radius:.5rem;box-shadow:0 4px 14px rgba(0,0,0,.25);opacity:0;transition:opacity .3s ease-in-out;pointer-events:none;font-size:.875rem}.toast.show{opacity:1}</style><script>function copyBibtex(){const e=document.getElementById("bibtex-content").innerText;navigator.clipboard.writeText(e).then(()=>{showToast("BibTeX copied to clipboard!")})}function showToast(e){const t=document.createElement("div");t.className="toast",t.textContent=e,document.body.appendChild(t),void t.offsetWidth,t.classList.add("show"),setTimeout(()=>{t.classList.remove("show"),setTimeout(()=>t.remove(),300)},3e3)}</script><ul id=taxonomy class="not-prose flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto"><li class="font-semibold my-4">Tags:</li><li><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300 uppercase" href=/tags/lowlight-enhancement/>Lowlight Enhancement</a></li><li><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300 uppercase" href=/tags/iccv/>Iccv</a></li><li><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300 uppercase" href=/tags/dataset/>Dataset</a></li></ul></article></div></main><footer class="bg-zinc-100 py-10 md:py-14"><script>const btn=document.getElementById("backToTop");window.addEventListener("scroll",()=>{btn.classList.toggle("hidden",window.scrollY<100)}),btn.addEventListener("click",()=>{window.scrollTo({top:0,behavior:"smooth"})})</script><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div class="flex flex-wrap space-y-6 mb-4"><div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10"><a class="flex items-center group" href=https://BUPTMMLab.github.io/><span class="text-4xl font-semibold uppercase">BUPTMMLab</span></a><p class=font-semibold>Research on Multimedia and Multimodal Learning.<br>Multimedia Lab, Beijing University of Posts and Telecommunications.</p></div><div class="self-center flex flex-col w-full md:w-2/5"><ul id=social-media class="flex items-center space-x-4"><li><a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href=https://github.com/BUPTMMLab target=_blank rel="noopener noreferrer"><i class="fa-brands fa-github fa-xl"></i></a></li></ul></div></div><div class=my-8><ul class="flex items-center space-x-4"><li><a class="decoration-auto hover:underline font-semibold" href=/>Home</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/publications/>Publications</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/tags/>Tags</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/members/>Members</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/about/>About</a></li></ul></div><div class="border-t pt-4"><p class=text-sm>Copyright Â© 2025. All rights reserved.</p></div></div><div class="text-center text-gray-500 text-sm mt-4 space-x-4"><span>æ¬ç«æ»è®¿é®é <span id=busuanzi_value_site_pv>0</span> æ¬¡</span>
<span>è®¿å®¢äººæ° <span id=busuanzi_value_site_uv>0</span> äºº</span></div><script src=/js/busuanzi.pure.mini.min.1c1e7fbaa98df67c212645a84aa3b7cbb1e5aeca16c576b935d9b8b89b6d7c55.js integrity="sha256-HB5/uqmN9nwhJkWoSqO3y7HlrsoWxXa5Ndm4uJttfFU=" defer></script></footer><script defer src=/js/insertoggle.474f9b0e08021c6519cff4e46df14ccf148285b2d3a23d6321d6e10f25c291fb.js integrity="sha256-R0+bDggCHGUZz/TkbfFMzxSChbLToj1jIdbhDyXCkfs=" crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){var e=document.createElement("div");e.className="adsbox",e.style.height="1px",e.style.position="absolute",e.style.top="-1000px",document.body.appendChild(e),window.setTimeout(function(){var t,n=e.offsetHeight===0;document.body.removeChild(e),n&&(t=document.createElement("div"),t.id="ublock-warning",t.style.position="fixed",t.style.top="0",t.style.left="0",t.style.width="100%",t.style.padding="12px",t.style.backgroundColor="#ffcc00",t.style.color="#000",t.style.fontSize="16px",t.style.textAlign="center",t.style.zIndex="9999",t.style.opacity="0",t.style.transition="opacity 1s ease",t.innerHTML="Detect the AD blocker. For a better browsing experience, please consider disabling uBlock Origin or other ad blockers on this site. Thank you!",document.body.appendChild(t),setTimeout(function(){t.style.opacity="1"},100),setTimeout(function(){t.style.opacity="0",setTimeout(function(){t.remove()},1e3)},5e3))},100)})</script></body></html>