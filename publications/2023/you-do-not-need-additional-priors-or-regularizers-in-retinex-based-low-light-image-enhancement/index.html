<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>You Do Not Need Additional Priors or Regularizers in Retinex-based Low-light Image Enhancement | BUPTMMLab</title><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.0/css/all.min.css><meta name=description content="This paper proposes a regularizer-free Retinex decomposition and synthesis network (RFR) for low-light image enhancement. It introduces a contrastive learning method and a self-knowledge distillation method to train the model without additional priors or regularizers. The approach extracts reflectance and illumination features and synthesizes them end-to-end, achieving superior performance on various datasets."><link rel=stylesheet href=/css/main.min.df34b0153fa1319eab0404d6d67f5c8cf3f0416e02003d820402c72b72984952.css integrity="sha256-3zSwFT+hMZ6rBATW1n9cjPPwQW4CAD2CBALHK3KYSVI=" crossorigin=anonymous><link rel=stylesheet href=/css/academicons.min.min.f6abb61f6b9b2e784eba22dfb93cd399ce30ee01825791830a2737d6bfcd2be9.css integrity="sha256-9qu2H2ubLnhOuiLfuTzTmc4w7gGCV5GDCic31r/NK+k=" crossorigin=anonymous><link rel=stylesheet href=/css/fontawesome-free/fontawesome.min.6fc54ad7f858066d7a20163a3700a27bf0d0db05f352e455209505c9f1cf0691.css integrity="sha256-b8VK1/hYBm16IBY6NwCie/DQ2wXzUuRVIJUFyfHPBpE=" crossorigin=anonymous><button id=backToTop class="w-12 h-12 fixed bottom-6 right-6 z-50 p-3 rounded-full bg-blue-600 text-white shadow-md hover:bg-blue-700 transition" aria-label="Back to top">
<i class="fa-solid fa-arrow-up"></i></button></head><body><header class="py-6 border-b"><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8 flex flex-col"><div class="flex items-center justify-between"><div class="flex items-center space-x-4"><img src=/images/BUPT_logo.png alt=Logo class="h-14 max-w-[20rem] object-contain"><div class="relative rounded-full py-1.5 px-6 hover:bg-zinc-200 text-xl font-bold uppercase"><h2><a class="before:content-[''] before:z-10 before:top-0 before:right-0 before:left-0 before:bottom-0 before:absolute before:pointer-events-auto text-4xl text-transparent bg-clip-text bg-gradient-to-r from-blue-500 to-purple-600" href=https://BUPTMMLab.github.io/>BUPTMMLab</a></h2></div></div><div class="flex items-center space-x-4"><a href=/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Home
</a><a href=/publications/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Publications
</a><a href=/tags/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Tags
</a><a href=/members/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">Members
</a><a href=/about/ class="text-l px-4 py-2 bg-zinc-100 hover:bg-zinc-200 rounded-full font-semibold text-black transition-colors">About</a></div></div></div></header><main class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div class="items-center justify-center"><article><header class="flex flex-col justify-center items-center mx-auto"><div class="flex flex-wrap justify-center gap-6 mb-2"><span class="inline-block text-white font-semibold px-4 py-1 rounded-lg text-2xl shadow-md select-none" style="background:linear-gradient(to right,#f472b6,#8b5cf6,#3b82f6);-webkit-background-clip:text;-webkit-text-fill-color:transparent">CVPR</span></div><h1 id=title class="text-4xl text-center font-bold leading-normal">You Do Not Need Additional Priors or Regularizers in Retinex-based Low-light Image Enhancement</h1><div class="text-gray-700 text-base font-medium flex flex-wrap justify-center mt-4"><a href=https://teacher.bupt.edu.cn/fuhuiyuan class="text-blue-600 hover:underline">Huiyuan Fu</a>
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">1</sup>,&nbsp;
Wenkai Zheng
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">1</sup>,&nbsp;
Xiangyu Meng
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">1</sup>,&nbsp;
Xin Wang
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">2</sup>,&nbsp;
Chuanming Wang
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">1</sup>,&nbsp;
<a href="https://scholar.google.com/citations?user=A-vcjvUAAAAJ" class="text-blue-600 hover:underline">Huadong Ma</a>
<sup class="align-top text-xs ml-0.5 text-gray-600 tracking-tight">1</sup></div><div class="mt-2 text-gray-600 text-sm text-center"><div><sup>1</sup> Beijing University of Posts and Telecommunications</div><div><sup>2</sup> Stony Brook University</div></div><div id=writer class="flex flex-col space-y-2"><span class="text-gray-700 text-base font-medium before:mr-2 before:opacity-50 space-x-4"><time datetime=2023-06-20T10:00:00+00:00>June 20, 2023</time></span></div><div class="flex flex-wrap justify-center gap-2 mt-6"><div><a href=https://openaccess.thecvf.com/content/CVPR2023/html/Fu_You_Do_Not_Need_Additional_Priors_or_Regularizers_in_Retinex-Based_CVPR_2023_paper.html class="inline-block flex items-center gap-2 bg-blue-100 text-blue-800 text-m font-semibold px-3 py-1 rounded-full hover:bg-blue-200 transition"><i class="fas fa-file-pdf"></i>
<span>Paper</span></a></div></div><br><div id=tldr-box class="bg-white border border-gray-300 rounded-xl shadow-md p-4 hover:shadow-xl transition"><p><strong>TL;DR:</strong> This paper proposes a regularizer-free Retinex decomposition and synthesis network (RFR) for low-light image enhancement. It introduces a contrastive learning method and a self-knowledge distillation method to train the model without additional priors or regularizers. The approach extracts reflectance and illumination features and synthesizes them end-to-end, achieving superior performance on various datasets.</p></div><br></header><figure id=featureimage class="flex justify-center"><img class="rounded-lg max-h-[500px] object-contain" src=https://BUPTMMLab.github.io/images/pubs/CVPR2023FuYouDoNotNeed/fu_cvpr_visual_comparison_hu_1d695c896953707b.png alt width=649 height=500></figure><div id=content class="prose max-w-none lg:prose-lg mb-8"><h2 id=abstract>Abstract</h2><p>Images captured in low-light conditions often suffer from significant quality degradation. Recent works have built a large variety of deep Retinex-based networks to enhance low-light images. The Retinex-based methods require decomposing the image into reflectance and illumination components, which is a highly ill-posed problem and there is no available ground truth. Previous works addressed this problem by imposing some additional priors or regularizers. However, finding an effective prior or regularizer that can be applied in various scenes is challenging, and the performance of the model suffers from too many additional constraints. We propose a contrastive learning method and a self-knowledge distillation method for Retinex decomposition that allow training our Retinex-based model without elaborate hand-crafted regularization functions. Rather than estimating reflectance and illuminance images and representing the final images as their element-wise products as in previous works, our regularizer-free Retinex decomposition and synthesis network (RFR) extracts reflectance and illuminance features and synthesizes them end-to-end. In addition, we propose a loss function for contrastive learning and a progressive learning strategy for self-knowledge distillation. Extensive experimental results demonstrate that our proposed methods can achieve superior performance compared with state-of-the-art approaches.</p><h2 id=network-architecture>Network Architecture</h2><div class="flex justify-center w-full not-prose"><figure class="w-full p-2 bg-gray-50 rounded-md shadow-sm text-center"><div class="inline-block m-0 w-4/5"><img src=/images/pubs/CVPR2023FuYouDoNotNeed/fu_cvpr_networkarch.png alt="RFR architecture" class="w-full h-auto object-cover rounded-md"></div><figcaption class="mt-2 text-m text-gray-600">The framework of our RFR, which consists of the reflectance module, the illumination module and the synthesis module. The reflectance module trained using our proposed contrastive learning method or self-knowledge distillation method extracts reflectance features, the illumination module extracts illumination features, and the synthesis module synthesizes them to generate the enhanced images.</figcaption></figure></div></div><div class="mt-12 mb-10 bg-white rounded-xl shadow-md" id=BibTeX><div class="flex items-center justify-between px-4 py-4 bg-white border-b border-gray-300"><span class="text-xl font-semibold text-black">BibTeX</span>
<button onclick=copyBibtex() class="text-sm bg-gray-200 hover:bg-gray-300 px-3 py-1 rounded flex items-center gap-2 text-black">
<i class="fa-solid fa-copy"></i>
<span>Copy</span></button></div><pre class="mt-3 px-4 py-2 bg-white text-black text-sm overflow-x-auto whitespace-pre-wrap break-words leading-snug">
<code id=bibtex-content style=white-space:pre class="block whitespace-pre-line break-words text-black">@inproceedings{FuYouDoNotNeedCVPR2023,
  author={Fu, Huiyuan and Zheng, Wenkai and Meng, Xiangyu and Wang, Xin and Wang, Chuanming and Ma, Huadong},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={You Do Not Need Additional Priors or Regularizers in Retinex-Based Low-Light Image Enhancement}, 
  year={2023},
}
</code>    
</pre></div><style>.toast{position:fixed;bottom:1.5rem;right:1.5rem;z-index:9999;background-color:#1f2937;color:#fff;padding:.75rem 1rem;border-radius:.5rem;box-shadow:0 4px 14px rgba(0,0,0,.25);opacity:0;transition:opacity .3s ease-in-out;pointer-events:none;font-size:.875rem}.toast.show{opacity:1}</style><script>function copyBibtex(){const e=document.getElementById("bibtex-content").innerText;navigator.clipboard.writeText(e).then(()=>{showToast("BibTeX copied to clipboard!")})}function showToast(e){const t=document.createElement("div");t.className="toast",t.textContent=e,document.body.appendChild(t),void t.offsetWidth,t.classList.add("show"),setTimeout(()=>{t.classList.remove("show"),setTimeout(()=>t.remove(),300)},3e3)}</script><ul id=taxonomy class="not-prose flex items-center space-x-4 flex-nowrap whitespace-nowrap overflow-x-auto"><li class="font-semibold my-4">Tags:</li><li><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300 uppercase" href=/tags/lowlight-enhancement/>Lowlight Enhancement</a></li><li><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300 uppercase" href=/tags/cvpr/>Cvpr</a></li><li><a class="py-2 px-6 border rounded-full hover:bg-zinc-100 active:bg-zinc-300 uppercase" href=/tags/retinex/>Retinex</a></li></ul></article></div></main><footer class="bg-zinc-100 py-10 md:py-14"><script>const btn=document.getElementById("backToTop");window.addEventListener("scroll",()=>{btn.classList.toggle("hidden",window.scrollY<100)}),btn.addEventListener("click",()=>{window.scrollTo({top:0,behavior:"smooth"})})</script><div class="max-w-7xl mx-auto px-4 md:px-6 lg:px-8"><div class="flex flex-wrap space-y-6 mb-4"><div class="w-full md:w-3/5 flex flex-col space-y-4 md:pr-8 lg:pr-10"><a class="flex items-center group" href=https://BUPTMMLab.github.io/><span class="text-4xl font-semibold uppercase">BUPTMMLab</span></a><p class=font-semibold>Research on Multimedia and Multimodal Learning.<br>Multimedia Lab, Beijing University of Posts and Telecommunications.</p></div><div class="self-center flex flex-col w-full md:w-2/5"><ul id=social-media class="flex items-center space-x-4"><li><a class="w-12 h-12 rounded-full bg-white hover:bg-zinc-200 flex items-center justify-center p-2" href=https://github.com/BUPTMMLab target=_blank rel="noopener noreferrer"><i class="fa-brands fa-github fa-xl"></i></a></li></ul></div></div><div class=my-8><ul class="flex items-center space-x-4"><li><a class="decoration-auto hover:underline font-semibold" href=/>Home</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/publications/>Publications</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/tags/>Tags</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/members/>Members</a></li><li><a class="decoration-auto hover:underline font-semibold" href=/about/>About</a></li></ul></div><div class="border-t pt-4"><p class=text-sm>Copyright © 2025. All rights reserved.</p></div></div><div class="text-center text-gray-500 text-sm mt-4 space-x-4"><span>本站总访问量 <span id=busuanzi_value_site_pv>0</span> 次</span>
<span>访客人数 <span id=busuanzi_value_site_uv>0</span> 人</span></div><script src=/js/busuanzi.pure.mini.min.1c1e7fbaa98df67c212645a84aa3b7cbb1e5aeca16c576b935d9b8b89b6d7c55.js integrity="sha256-HB5/uqmN9nwhJkWoSqO3y7HlrsoWxXa5Ndm4uJttfFU=" defer></script></footer><script defer src=/js/insertoggle.474f9b0e08021c6519cff4e46df14ccf148285b2d3a23d6321d6e10f25c291fb.js integrity="sha256-R0+bDggCHGUZz/TkbfFMzxSChbLToj1jIdbhDyXCkfs=" crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){var e=document.createElement("div");e.className="adsbox",e.style.height="1px",e.style.position="absolute",e.style.top="-1000px",document.body.appendChild(e),window.setTimeout(function(){var t,n=e.offsetHeight===0;document.body.removeChild(e),n&&(t=document.createElement("div"),t.id="ublock-warning",t.style.position="fixed",t.style.top="0",t.style.left="0",t.style.width="100%",t.style.padding="12px",t.style.backgroundColor="#ffcc00",t.style.color="#000",t.style.fontSize="16px",t.style.textAlign="center",t.style.zIndex="9999",t.style.opacity="0",t.style.transition="opacity 1s ease",t.innerHTML="Detect the AD blocker. For a better browsing experience, please consider disabling uBlock Origin or other ad blockers on this site. Thank you!",document.body.appendChild(t),setTimeout(function(){t.style.opacity="1"},100),setTimeout(function(){t.style.opacity="0",setTimeout(function(){t.remove()},1e3)},5e3))},100)})</script></body></html>