<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>BUPTMMLab</title><link>https://BUPTMMLab.github.io/</link><description>Recent content on BUPTMMLab</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 26 Jun 2025 10:00:00 +0000</lastBuildDate><atom:link href="https://BUPTMMLab.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>From Abyssal Darkness to Blinding Glare: A Benchmark on Extreme Exposure Correction in Real World</title><link>https://BUPTMMLab.github.io/publications/2025/from-abyssal-darkness-to-blinding-glare/</link><pubDate>Thu, 26 Jun 2025 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2025/from-abyssal-darkness-to-blinding-glare/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Exposure correction aims to restore over/under-exposed images to well-exposed ones using a single network. However, existing methods mainly handle non-extreme exposure conditions and struggle with the severe luminance and texture loss caused by extreme exposure. Through a thorough investigation, we find that the lack of high-quality benchmark datasets significantly limits progress in extreme exposure correction. To address this issue, we introduce the first Real-world Extreme Exposure Dataset, REED. By leveraging the burst shooting mode of cameras, we capture image sequences covering a luminance range from extremely dark to extremely bright. To prevent misalignment caused by camera motion and scene changes, we apply cropping and an improved SIFT algorithm to ensure precise alignment. We also propose a novel Context-Guided Luminance-Normalized Iterative Exposure Refinement Network. We employ Contrastive Loss and Luminance Normalizer to disentangle the coupled distribution of over/under-exposed images. In certain cases, luminance alone is insufficient for determining over/under-exposure, so we integrate semantic guidance into the Semantic-aware Exposure Diffusion Model to further enhance luminance and texture restoration. Inspired by the effectiveness of iterative correction in improving color and texture, we introduce the CLIP-Guided Iterative Refinement Strategy. Extensive experiments validate the superiority of our dataset and method. Our dataset and code will be publicly available.&lt;/p></description></item><item><title>About BUPT MMLab</title><link>https://BUPTMMLab.github.io/about/</link><pubDate>Sat, 07 Jun 2025 17:31:29 +0800</pubDate><guid>https://BUPTMMLab.github.io/about/</guid><description>&lt;p>Work in progress&amp;hellip;&lt;/p></description></item><item><title>Team Members in MMLab</title><link>https://BUPTMMLab.github.io/members/</link><pubDate>Thu, 05 Jun 2025 19:33:14 +0800</pubDate><guid>https://BUPTMMLab.github.io/members/</guid><description>&lt;p>Work in progress&amp;hellip;&lt;/p></description></item><item><title>Exploring in Extremely Dark: Low-Light Video Enhancement with Real Events</title><link>https://BUPTMMLab.github.io/publications/2024/exploring-in-extremely-dark-low-light-video-enhancement-with-real-events/</link><pubDate>Mon, 28 Oct 2024 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2024/exploring-in-extremely-dark-low-light-video-enhancement-with-real-events/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Due to the limitations of sensor, traditional cameras struggle to capture details within extremely dark areas of videos. The absence of such details can significantly impact the effectiveness of low-light video enhancement. In contrast, event cameras offer a visual representation with higher dynamic range, facilitating the capture of motion information even in exceptionally dark conditions. Motivated by this advantage, we propose the Real-Event Embedded Network for low-light video enhancement. To better utilize events for enhancing extremely dark regions, we propose an Event-Image Fusion module, which can identify these dark regions and enhance them significantly. To ensure temporal stability of the video and restore details within extremely dark areas, we design unsupervised temporal consistency loss and detail contrast loss. Alongside the supervised loss, these loss functions collectively contribute to the semi-supervised training of the network on unpaired real data. Experimental results on synthetic and real data demonstrate the superiority of the proposed method compared to the state-of-the-art methods.&lt;/p></description></item><item><title>Learning Exposure Correction in Dynamic Scenes</title><link>https://BUPTMMLab.github.io/publications/2024/learning-exposure-correction-in-dynamic-scenes/</link><pubDate>Mon, 28 Oct 2024 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2024/learning-exposure-correction-in-dynamic-scenes/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Exposure correction aims to enhance visual data suffering from improper exposures, which can greatly improve satisfactory visual effects. However, previous methods mainly focus on the image modality, and the video counterpart is less explored in the literature. Directly applying prior image-based methods to videos results in temporal incoherence with low visual quality. Through thorough investigation, we find that the development of relevant communities is limited by the absence of a benchmark dataset. Therefore, in this paper, we construct the first real-world paired video dataset, including both underexposure and overexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR cameras and a beam splitter to simultaneously capture improper and normal exposure videos. Additionally, we propose an end-to-end video exposure correction network, in which a dual-stream module is designed to deal with both underexposure and overexposure factors, enhancing the illumination based on Retinex theory. The extensive experiments based on various metrics and user studies demonstrate the significance of our dataset and the effectiveness of our method. The code and dataset are available at &lt;a href="https://github.com/kravrolens/VECNet">https://github.com/kravrolens/VECNet&lt;/a>.&lt;/p></description></item><item><title>Region-Aware Exposure Consistency Network for Mixed Exposure Correction</title><link>https://BUPTMMLab.github.io/publications/2024/region-aware-exposure-consistency-network-for-mixed-exposure-correction/</link><pubDate>Wed, 28 Feb 2024 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2024/region-aware-exposure-consistency-network-for-mixed-exposure-correction/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Exposure correction aims to enhance images suffering from improper exposure to achieve satisfactory visual effects. Despite recent progress, existing methods generally mitigate either overexposure or underexposure in input images, and they still struggle to handle images with mixed exposure, i.e., one image incorporates both overexposed and underexposed regions. The mixed exposure distribution is non-uniform and leads to varying representation, which makes it challenging to address in a unified process. In this paper, we introduce an effective Region-aware Exposure Correction Network (RECNet) that can handle mixed exposure by adaptively learning and bridging different regional exposure representations. Specifically, to address the challenge posed by mixed exposure disparities, we develop a region-aware de-exposure module that effectively translates regional features of mixed exposure scenarios into an exposure-invariant feature space. Simultaneously, as de-exposure operation inevitably reduces discriminative information, we introduce a mixed-scale restoration unit that integrates exposure-invariant features and unprocessed features to recover local information. To further achieve a uniform exposure distribution in the global image, we propose an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity. Extensive experiments are conducted on various datasets, and the experimental results demonstrate the superiority and generalization of our proposed method. The code is released at: &lt;a href="https://github.com/kravrolens/RECNet">https://github.com/kravrolens/RECNet&lt;/a>.&lt;/p></description></item><item><title>Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement</title><link>https://BUPTMMLab.github.io/publications/2023/dancing-in-the-dark-a-benchmark-towards-general-low-light-video-enhancement/</link><pubDate>Sun, 08 Oct 2023 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2023/dancing-in-the-dark-a-benchmark-towards-general-low-light-video-enhancement/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Low-light video enhancement is a challenging task with broad applications. However, current research in this area is limited by the lack of high-quality benchmark datasets. To address this issue, we design a camera system and collect a high-quality low-light video dataset with multiple exposures and cameras. Our dataset provides dynamic video pairs with pronounced camera motion and strict spatial alignment. To achieve general low-light video enhancement, we also propose a novel Retinex-based method named Light Adjustable Network (LAN). LAN iteratively refines the illumination and adaptively adjusts it under varying lighting conditions, leading to visually appealing results even in diverse real-world scenarios. The extensive experiments demonstrate the superiority of our low-light video dataset and enhancement method. Our dataset is available at &lt;a href="https://github.com/ciki000/DID">https://github.com/ciki000/DID&lt;/a>.&lt;/p></description></item><item><title>You Do Not Need Additional Priors or Regularizers in Retinex-based Low-light Image Enhancement</title><link>https://BUPTMMLab.github.io/publications/2023/you-do-not-need-additional-priors-or-regularizers-in-retinex-based-low-light-image-enhancement/</link><pubDate>Sun, 18 Jun 2023 10:00:00 +0000</pubDate><guid>https://BUPTMMLab.github.io/publications/2023/you-do-not-need-additional-priors-or-regularizers-in-retinex-based-low-light-image-enhancement/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Images captured in low-light conditions often suffer from significant quality degradation. Recent works have built a large variety of deep Retinex-based networks to enhance low-light images. The Retinex-based methods require decomposing the image into reflectance and illumination components, which is a highly ill-posed problem and there is no available ground truth. Previous works addressed this problem by imposing some additional priors or regularizers. However, finding an effective prior or regularizer that can be applied in various scenes is challenging, and the performance of the model suffers from too many additional constraints. We propose a contrastive learning method and a self-knowledge distillation method for Retinex decomposition that allow training our Retinex-based model without elaborate hand-crafted regularization functions. Rather than estimating reflectance and illuminance images and representing the final images as their element-wise products as in previous works, our regularizer-free Retinex decomposition and synthesis network (RFR) extracts reflectance and illuminance features and synthesizes them end-to-end. In addition, we propose a loss function for contrastive learning and a progressive learning strategy for self-knowledge distillation. Extensive experimental results demonstrate that our proposed methods can achieve superior performance compared with state-of-the-art approaches.&lt;/p></description></item></channel></rss>